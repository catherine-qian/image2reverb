# room2reverb

![room2reverb architecture](/readme/r2rarch.png)
room2reverb is a deep neural net model that directly synthesizes IRs from visual images of acoustic environments. The architecture is a conditional GAN built using PyTorch and using ResNet50 (pre-trainedon Places365) as an encoder. room2reverb is a fully  end-to-end acoustic impulse response generator currently being developed as part of the Applied Machine Learning course 6.862 at MIT.

## Dependencies

PyTorch (including torchvision, torchaudio), librosa.

Matlab is required to run the custom scripts used for calculating metrics and statistics of generated IRs, but not for the model itself.  

## Usage

The pre-trained model will be uploaded and available for use.

Examples of scenes used to train our top-performing model:


![input samples](/readme/inputs.png)

## Why are we excited about room2reverb?

An effective and widely used method of simulating acoustic spaces relies on audio impulse responses (IRs) and convolution (Valimaki et al., 2012; Robjohns, 1999). Audio IRs are recorded measurements of how an environment responds to an acoustic stimulus. IRs can be measured by recording a space during a burst of white noise like a clap or balloon pop or a sinusoid swept across the range of human hearing (Reilly & McGrath, 1995). Accurately capturing these room impulse responses requires time, specialized equipment, knowledge, and planning. Directly recording these measurements may be entirely infeasible in continuously inhabited spaces of interest. End-to-end IR estimation has far ranging applications relevant to fields including music production, speech processing, and generating immersive extended reality (XR) environments. Considering the high costs yet far-reaching potential benefits of easily obtaining these measurements, here we demonstrate a model that directly synthesizes IRs from visual images of acoustic environments.

## Documentation

More details will be available once the class project is complete.

## License
